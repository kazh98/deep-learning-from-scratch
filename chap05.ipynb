{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.9698151289792513e-10\n",
      "b1:2.4430850746818136e-09\n",
      "W2:7.306421356617458e-09\n",
      "b2:1.395481852198288e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from misc.neuralnet import TwoLayerNet\n",
    "from misc.mnist import load_train_data\n",
    "\n",
    "X_train, y_train = load_train_data(True)\n",
    "X_train = X_train.astype(np.float_) / 255\n",
    "\n",
    "network = TwoLayerNet(784, 50, 10)\n",
    "X_batch = X_train[:3]\n",
    "y_batch = y_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(X_batch, y_batch)\n",
    "grad_backprop = network.gradient(X_batch, y_batch)\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + ':' + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0:   5.10833333 (2.30298820)\n",
      "  600:  50.59333333 (1.99929746)\n",
      " 1200:  79.00666667 (0.89139679)\n",
      " 1800:  84.57333333 (0.59263677)\n",
      " 2400:  87.14000000 (0.48477820)\n",
      " 3000:  88.26166667 (0.43016099)\n",
      " 3600:  89.05833333 (0.39641585)\n",
      " 4200:  89.56166667 (0.37302795)\n",
      " 4800:  89.89833333 (0.35622458)\n",
      " 5400:  90.37166667 (0.34244861)\n",
      " 6000:  90.68333333 (0.33062081)\n",
      " 6600:  90.93166667 (0.32133308)\n",
      " 7200:  91.15333333 (0.31286099)\n",
      " 7800:  91.38666667 (0.30429227)\n",
      " 8400:  91.57166667 (0.29803324)\n",
      " 9000:  91.76500000 (0.29135719)\n",
      " 9600:  91.95000000 (0.28542555)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.36999999999999"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from misc.neuralnet import TwoLayerNet\n",
    "from misc.mnist import load_train_data, load_test_data\n",
    "\n",
    "n_iter = 10000\n",
    "batch_size = 100\n",
    "alpha = 0.01\n",
    "\n",
    "X_train, y_train = load_train_data(True)\n",
    "X_test, y_test = load_test_data(True)\n",
    "X_train = X_train.astype(np.float_) / 255\n",
    "X_test = X_test.astype(np.float_) / 255\n",
    "\n",
    "network = TwoLayerNet(784, 50, 10)\n",
    "for k in range(n_iter):\n",
    "    batch_mask = np.random.choice(X_train.shape[0], batch_size)\n",
    "    X_batch = X_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(X_batch, y_batch)\n",
    "    for key in grad.keys():\n",
    "        network.params[key] -= alpha * grad[key]\n",
    "        \n",
    "    if k % (X_train.shape[0] / batch_size) == 0:\n",
    "        print('%5d: %12.8f (%.8f)' % (k, network.accuracy(X_train, y_train) * 100, network.loss(X_train, y_train)), flush=True)\n",
    "        \n",
    "network.accuracy(X_test, y_test) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
